[Trainer]
output_dir=${Save:output_dir}
report_to= tensorboard
seed= 666
per_device_train_batch_size= 4
per_device_eval_batch_size= 4
gradient_accumulation_steps= 1
fp16= True
do_eval = True
do_train = True
evaluation_strategy= epoch
# eval_steps= 500
load_best_model_at_end= True
save_strategy= epoch
# save_steps= 500
num_train_epochs= 5
save_total_limit= 2
learning_rate= 2e-5
metric_for_best_model= eval_f1-score
warmup_ratio= 0.06
label_names= [cls_labels]



[Save]
postfix= debug
output_dir= output_dir/output_${postfix}
best_model_file = ${output_dir}/best_model/best.pt
temp_dir= ${output_dir}/temp_dir
vocab_file = ${temp_dir}/vocab.pkl
load_from_pkl= False
log_file= ${output_dir}/log_${postfix}.txt

[data]
pretrained_model_name_or_path = hfl/chinese-roberta-wwm-ext

train_file= [data/ori_data/train/train_data.csv,data/ori_data/train/train_large.json]
dev_file= [data/ori_data/dev/dev_data.csv]
test_file= [data/ori_data/test/testa.csv]
max_len = 430
max_train_num = 300000
topk = 10
